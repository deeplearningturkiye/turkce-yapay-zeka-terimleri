# SÖZLÜKTE YER ALMAYANLAR
Aşağıdaki kelime ya da kelime gruplarının Türkçe karşılıkları sözlükte yer almamaktadır.

## adversarial
- [ ] TÜBA
- [ ] TDK

ÖNERİ: `düşman, hasım, çekişmeli`

## autoencoder
- [ ] TÜBA
- [ ] TDK

**Encoder** kelimesinin TDK sözlüğünde karşılığı **kodlayıcı** olarak belirtilmektedir. 

ÖNERİ: `otokodlayıcı`

## bounding box
- [ ] TÜBA
- [ ] TDK

ÖNERİ: `sınırlayıcı kutu`

## capsule
- [x] TÜBA
- [x] TDK

ÖNERİ: `kapsül`

## computer vision
- [x] TÜBA
- [ ] TDK

ÖNERİ: `bilgisayarlı görme, bilgisayarlı görü`

## coarse grain
- [x] TÜBA
- [x] TDK

ÖNERİ: `iri taneli`

## convolutional neural networks
- [x] TÜBA
- [x] TDK

Sözlükte **convolution**, **evrişim** olarak çevrilmiştir.

ÖNERİ: `evrişimli sinir ağları`

## dropout
- [x] TÜBA
- [ ] TDK

**dropout** TÜBA'da **güç sönümü** olarak geçmektedir. Yapay zeka alanında elektronikle ilgili bu çevirinin uygun olmadığı düşünülmektedir.

ÖNERİ: `seyreltme`

## dynamic routing
- [x] TÜBA
- [x] TDK

**routing*** kelimesi TÜBA'da **rotalama** TDK'da ise **yöneltilme** olarak çevrilmiştir.

ÖNERİ: `dinamik yönlendirme, dinamik rotalama`

## embedding
- [ ] TÜBA
- [ ] TDK

ÖNERİ: `özyerleşik` - Hacettepe Unv. Multimedia Information Retrieval Lab önerisidir.

## feature representation 
- [ ] TÜBA
- [ ] TDK

**feature** TDK'da **özellik** TÜBA'da ise **öznitelik** olarak çevrilmiştir.

ÖNERİ: `öznitelik temsili`

## fine grain 
- [ ] TÜBA
- [ ] TDK

## fine tuning
- [ ] TÜBA
- [ ] TDK

## framework
- [x] TÜBA
- [x] TDK

**framework** TDK'da ve TÜBA'da **çatı** şeklinde çevrilmiştir.

ÖNERİ: `çatı`

## generative adversarial networks
- [ ] TÜBA
- [ ] TDK

[IEEE Sinyal İşleme ve İletişim Uygulamaları Kurultayı 2017 (SIU)](https://www.openconf.org/siu2017/modules/request.php?module=oc_program&action=summary.php&id=655
) ve [Bozkırda Yapay Öğrenme Yaz Okulu 2017](https://byoyo2017.vision.cs.hacettepe.edu.tr/lectures.html)'de 2014 yılında Ian Goodfellow tarafından keşfedilen Generative Adversarial Networks (GAN), Türkçeye Çekişmeli Üretici Ağlar olarak çevrilmiştir.

ÖNERİ: `çekişmeli üretici ağlar`

## ground truth
- [ ] TÜBA
- [ ] TDK

**ground truth** TDK'da **kesin referans** TÜBA'da **gerçek referans değer** şeklinde çevrilmiştir.

ÖNERİ: `gerçek referans değer`

## hyperparameter
- [ ] TÜBA
- [ ] TDK

## landmark detection 
- [x] TÜBA
- [ ] TDK

ÖNERİ: `karakteristik nokta saptama`

## latent concept 
- [ ] TÜBA
- [ ] TDK

## localization 
- [x] TÜBA
- [ ] TDK

ÖNERİ: `yerseme (yerini saptama, lokalizasyon)`

## padding 
- [ ] TÜBA
- [ ] TDK

ÖNERİ: `dolgulama`

## perplexity
- [ ] TÜBA
- [ ] TDK

## pooling
- [ ] TÜBA
- [ ] TDK

## predictor
- [x] TÜBA
- [x] TDK

**predictor** TDK'da **yordamlayıcı** TÜBA'da **bağımsız değişken** şeklinde çevrilmiştir.

ÖNERİ: `bağımsız değişken` 

## semantics
- [ ] TÜBA
- [ ] TDK

## squashing function
- [ ] TÜBA
- [ ] TDK

# SÖZLÜKTE YER ALAN FAKAT DEĞİŞTİRİLMESİ ÖNERİLENLER
Aşağıdaki kelime ya da kelime gruplarının Türkçe karşılıkları sözlükte yer almaktadır. Fakat ufak değişikliklerin yapılması gerekmektedir. 

## condensed
- [ ] TÜBA
- [x] TDK

**condense** kelimesinin karşılığı TDK'da **yoğunlaştırmak** olarak geçmektedir. Sözlükte ise **seyrek** olarak çevrildiği görülmektedir.

ÖNERİ: `yoğun`

## epoch
- [ ] TÜBA
- [x] TDK

**epoch** TDK'da **devre** olarak geçmektedir. Sözlük'te ise **dönem** olarak çevrilmiştir. Makine öğrenmesinde bir verisetinin tamamının bir kere ağdan geçiş yapmasına İngilizcede **epoch** denilmektedir.

ÖNERİ: `devir`

## np-complete

**np-complete** ingilizce tanımı: problems that are solved in polynomial time by a non-deterministic turing machine. Türkçe çevirisinde ise "çokterimli zamanda bulunamaz" şeklinde geçiyor. Fakat bu yanlış bir kullanım. NP sınıfı çoğu zaman "non-polynomial time" şeklinde karıştırılıyor. Fakat asıl açılımı "non-deterministic polynomial time". Biz henüz bu sınıftaki problemlerin çokterimli zamanda bulunamayacağını kanıtlayamadık.

NP-complete sınıfını NP sınıfından ayıran fark ise NP sınıfındaki her problemi çokterimli zaman kullanarak NP-complete sınıfındaki bir probleme eşleyebilmemiz. Bu nedenle NP-complete sınıfındaki herhangi bir probleme çokterimli zamanda bir çözüm üretirsek, P=NP doğru oluyor.

Bu sınıfların harf kısaltmaları aynı şekilde kullanılabilir. Doğal sayı kümesine N dediğimiz gibi.

ÖNERİ: `NP-bütün`



RNN(Recurrent Nerual Network)
RNN 1980’ de geliştirildi. Makine Öğrenmesinde Derin Öğrenme algoritmalarında biri olarak kullanılır. Tanım olarak
, ardışık bilgileri kullanan modeldir. Kullanım Alanları; Konuşma tanıma, Makine Çevirisi vb.
ÖNERİ: "Tekrarlayan Sinir Ağları"


